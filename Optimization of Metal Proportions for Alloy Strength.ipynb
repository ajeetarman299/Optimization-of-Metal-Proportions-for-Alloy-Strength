{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KScvOYUXHvrD",
        "outputId": "a5b818b6-5ff0-4f0d-9bff-969f499de3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoOuCsTuSRli"
      },
      "outputs": [],
      "source": [
        "# import  header\n",
        "import random\n",
        "random.seed(1)\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.max_columns', 20)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "\n",
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import Ridge,Lasso,ElasticNet,SGDRegressor,LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# from base_function.base import *\n",
        "\n",
        "path = %pwd\n",
        "\n",
        "def mean_relative_error(y_true, y_pred):\n",
        "    import numpy as np\n",
        "    relative_error = np.average(np.abs(y_true - y_pred) / y_true, axis=0)\n",
        "    return relative_error\n",
        "\n",
        "def Linear_SVR(C=1.0,gamma=0.1,epsilon=1):\n",
        "    return Pipeline([\n",
        "        (\"std_scaler\",StandardScaler()),\n",
        "        (\"model\",SVR(kernel=\"linear\",C=C,gamma=gamma,epsilon=epsilon))\n",
        "    ])\n",
        "def RBF_SVR(C=1.0,gamma=1,epsilon=1):\n",
        "    return Pipeline([\n",
        "        (\"std_scaler\",StandardScaler()),\n",
        "        (\"model\",SVR(kernel=\"rbf\",C=C,gamma=gamma,epsilon=epsilon))\n",
        "    ])\n",
        "def Poly_LinearRegression(degree=2):\n",
        "    return Pipeline([('poly', PolynomialFeatures(degree=degree)),\n",
        "                  ('linear', LinearRegression())])\n",
        "def draw_feature_importance(features,feature_importance):\n",
        "    \"\"\"\n",
        "    features: name\n",
        "    feature_importance:\n",
        "    \"\"\"\n",
        "    # make importances relative to max importance\n",
        "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
        "    sorted_idx = np.argsort(feature_importance)\n",
        "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
        "    fig = plt.figure(dpi=400)\n",
        "    plt.barh(pos, list(feature_importance[sorted_idx]), align='center')\n",
        "    plt.yticks(pos, list(features[sorted_idx]))\n",
        "    plt.xlabel('Relative Importance')\n",
        "    plt.title('Feature Importance')\n",
        "    plt.show()\n",
        "def model_fit_evaluation(model, x_train, y_train, x_test, y_test, n_fold=5):\n",
        "    \"\"\"clf:\n",
        "    x_train：\n",
        "    kf = KFold(n_splits=n_fold,shuffle=True,random_state=0)\n",
        "    print(model)\n",
        "    result = pd.DataFrame()\n",
        "    for i, (train_index, test_index) in enumerate(kf.split(range(len(x_train)))):\n",
        "        x_tr = x_train[train_index]\n",
        "        y_tr = y_train[train_index]\n",
        "        x_validation = x_train[test_index]  # get validation set\n",
        "        y_validation = y_train[test_index]\n",
        "        model.fit(x_tr, y_tr)\n",
        "\n",
        "        result_subset = pd.DataFrame()  # save the prediction\n",
        "        result_subset[\"y_validation\"] = y_validation\n",
        "        result_subset[\"y_pred\"] = model.predict(x_validation)\n",
        "        result = result.append(result_subset)\n",
        "    print(\"cross_validation_error in validation set：\")\n",
        "    c = evaluate_model_plot(result[\"y_validation\"],result[\"y_pred\"],show=False)\n",
        "\n",
        "    print(\"error in testing set：\")\n",
        "    model.fit(x_train, y_train)\n",
        "    y_test_pred = model.predict(x_test)\n",
        "    error_metric_testing = evaluate_model_plot(y_test,y_test_pred,show=False)\n",
        "    print(\"====================================\")\n",
        "    return error_metric_testing\n",
        "def fatigue_mre_metric(y_true, y_predict):\n",
        "    \"\"\"\n",
        "    calculate the mre of fatigue life\n",
        "    \"\"\"\n",
        "    return mean_relative_error(np.power(10,y_true), np.power(10,y_predict))\n",
        "# LOGCV for selection features\n",
        "def get_score_logcv(X_train,Y_train,groups_array_train,model):\n",
        "    \"\"\"\n",
        "    groups_array_train:groups of S-N curve\n",
        "    return the MRE of LOGCV\n",
        "    \"\"\"\n",
        "    logo = LeaveOneGroupOut()\n",
        "    y_predict_list = []\n",
        "    y_true_list = []\n",
        "    index_list = []\n",
        "\n",
        "    x = X_train.values\n",
        "    y = Y_train.values\n",
        "    for train_index, test_index in logo.split(x, y, groups_array_train):# LOOGV\n",
        "        x_train, x_test = x[train_index], x[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        model.fit(x_train,y_train)\n",
        "        y_predict_list.extend(model.predict(x_test))\n",
        "        y_true_list.extend(y_test)\n",
        "        index_list.extend(index[test_index])\n",
        "\n",
        "    # calculate metric\n",
        "    mre = mean_relative_error(np.power(10,y_true_list), np.power(10,y_predict_list))\n",
        "    return mre"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 reading data\n",
        "dataset = pd.read_excel('/content/data (1).xlsx',sheet_name=\"fatigue_strength\")\n",
        "# extract tempers\n",
        "condition_list = []\n",
        "# same alloy with different temps has same name\n",
        "name_list=[]\n",
        "for i in dataset[\"url\"]:\n",
        "    #print(i)\n",
        "    condition = re.findall(\"-([OF]|[HT]\\d+)-\",i)[0]\n",
        "    condition_list.append(condition)\n",
        "    name = i.replace(r\"https://www.makeitfrom.com/material-properties/\",\"\")\n",
        "    name = name.replace(condition,\"\")\n",
        "    name_list.append(name)\n",
        "dataset[\"tempers\"] = condition_list\n",
        "dataset = dataset.drop([\"url\"],axis=1)\n",
        "dataset = dataset.fillna(0)\n",
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "GhR2OSelS3f6",
        "outputId": "523ea12e-b8e2-41b0-a75b-a5fc9da3018f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Al     Fe     Si     Zn     Cu      V   Zr   Ag     Mn   Ni  ...  \\\n",
              "0  99.750  0.200  0.120  0.030  0.030  0.030  0.0  0.0  0.030  0.0  ...   \n",
              "1  99.800  0.175  0.125  0.025  0.025  0.025  0.0  0.0  0.015  0.0  ...   \n",
              "2  99.650  0.350  0.350  0.025  0.050  0.000  0.0  0.0  0.025  0.0  ...   \n",
              "3  99.525  0.400  0.075  0.050  0.010  0.000  0.0  0.0  0.025  0.0  ...   \n",
              "4  99.525  0.400  0.075  0.050  0.010  0.000  0.0  0.0  0.025  0.0  ...   \n",
              "\n",
              "      Mg    B   Ga   Cr   Pb   Bi   Li   Co  Fatigue Strength  tempers  \n",
              "0  0.030  0.0  0.0  0.0  0.0  0.0  0.0  0.0                31     H112  \n",
              "1  0.015  0.0  0.0  0.0  0.0  0.0  0.0  0.0                15     H112  \n",
              "2  0.025  0.0  0.0  0.0  0.0  0.0  0.0  0.0                47     H112  \n",
              "3  0.025  0.0  0.0  0.0  0.0  0.0  0.0  0.0                42      H12  \n",
              "4  0.025  0.0  0.0  0.0  0.0  0.0  0.0  0.0                49      H14  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bdb73748-769b-4433-9a7a-52363042f5a8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Al</th>\n",
              "      <th>Fe</th>\n",
              "      <th>Si</th>\n",
              "      <th>Zn</th>\n",
              "      <th>Cu</th>\n",
              "      <th>V</th>\n",
              "      <th>Zr</th>\n",
              "      <th>Ag</th>\n",
              "      <th>Mn</th>\n",
              "      <th>Ni</th>\n",
              "      <th>...</th>\n",
              "      <th>Mg</th>\n",
              "      <th>B</th>\n",
              "      <th>Ga</th>\n",
              "      <th>Cr</th>\n",
              "      <th>Pb</th>\n",
              "      <th>Bi</th>\n",
              "      <th>Li</th>\n",
              "      <th>Co</th>\n",
              "      <th>Fatigue Strength</th>\n",
              "      <th>tempers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>99.750</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31</td>\n",
              "      <td>H112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>99.800</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15</td>\n",
              "      <td>H112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>99.650</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47</td>\n",
              "      <td>H112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>99.525</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42</td>\n",
              "      <td>H12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>99.525</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49</td>\n",
              "      <td>H14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bdb73748-769b-4433-9a7a-52363042f5a8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bdb73748-769b-4433-9a7a-52363042f5a8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bdb73748-769b-4433-9a7a-52363042f5a8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-657cde42-027c-448d-931a-a846e08ddabe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-657cde42-027c-448d-931a-a846e08ddabe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-657cde42-027c-448d-931a-a846e08ddabe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 data processing\n",
        "# condition label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "dataset[\"tempers\"]=le.fit_transform(dataset[\"tempers\"])\n",
        "\n",
        "# save the LabelEncoder\n",
        "import pickle\n",
        "with open (\"le_tempers.txt\", 'wb') as f:\n",
        "    pickle.dump(le, f)"
      ],
      "metadata": {
        "id": "Xy1tQvWgTKWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 machine leanrning model\n",
        "Y_columns = \"Fatigue Strength\"\n",
        "X = dataset.drop([Y_columns],axis=1).values\n",
        "Y = dataset[Y_columns].values\n",
        "X_columns = dataset.drop([Y_columns],axis=1).columns"
      ],
      "metadata": {
        "id": "mVmN8197TPSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysx9HOhSJlcQ",
        "outputId": "cd6fdbbd-49bc-43e2-d9e0-c132379ea027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9750e+01, 2.0000e-01, 1.2000e-01, 3.0000e-02, 3.0000e-02,\n",
              "        3.0000e-02, 0.0000e+00, 0.0000e+00, 3.0000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.0000e-02, 3.0000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 3.0000e+00],\n",
              "       [9.9800e+01, 1.7500e-01, 1.2500e-01, 2.5000e-02, 2.5000e-02,\n",
              "        2.5000e-02, 0.0000e+00, 0.0000e+00, 1.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.5000e-02, 1.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 3.0000e+00],\n",
              "       [9.9650e+01, 3.5000e-01, 3.5000e-01, 2.5000e-02, 5.0000e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 3.0000e+00],\n",
              "       [9.9525e+01, 4.0000e-01, 7.5000e-02, 5.0000e-02, 1.0000e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.5000e-02, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 5.0000e+00],\n",
              "       [9.9525e+01, 4.0000e-01, 7.5000e-02, 5.0000e-02, 1.0000e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.5000e-02, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 7.0000e+00],\n",
              "       [9.9525e+01, 4.0000e-01, 7.5000e-02, 5.0000e-02, 1.0000e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.5000e-02, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 9.0000e+00],\n",
              "       [9.9750e+01, 2.0000e-01, 1.2500e-01, 2.5000e-02, 2.5000e-02,\n",
              "        2.5000e-02, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.5000e-02, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 1.4000e+01],\n",
              "       [9.9750e+01, 2.0000e-01, 1.2500e-01, 2.5000e-02, 2.5000e-02,\n",
              "        2.5000e-02, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.5000e-02, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 9.0000e+00],\n",
              "       [9.9750e+01, 2.0000e-01, 1.2500e-01, 2.5000e-02, 2.5000e-02,\n",
              "        2.5000e-02, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.5000e-02, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 5.0000e+00],\n",
              "       [9.9750e+01, 2.0000e-01, 1.2500e-01, 2.5000e-02, 2.5000e-02,\n",
              "        2.5000e-02, 0.0000e+00, 0.0000e+00, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.5000e-02, 2.5000e-02, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 2.7000e+01]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test =train_test_split(X,Y,test_size=0.3,random_state=0)"
      ],
      "metadata": {
        "id": "qfKuS3R1TTYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper-paramerer\n",
        "def hyper_param_opt_model(X, y,model,param_grid):\n",
        "    \"\"\"\n",
        "    hyper-paramerer optimize by GridSearch\n",
        "    \"\"\"\n",
        "    gsc = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid,\n",
        "        cv=5,\n",
        "        scoring= 'neg_mean_squared_error',\n",
        "        verbose=0,\n",
        "        n_jobs=-1)\n",
        "\n",
        "    grid_result = gsc.fit(X, y)\n",
        "    best_params = grid_result.best_params_\n",
        "    return best_params\n",
        "\n",
        "print('The best parameters: ',hyper_param_opt_model(X_train,Y_train,RandomForestRegressor(random_state=1),param_grid={\n",
        "            'max_depth': range(11,16),\n",
        "            'n_estimators': (5,50,100,200,1000)}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaIv_rmFTV3c",
        "outputId": "ffd82600-a251-4578-9847-9da3a278f175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best parameters:  {'max_depth': 15, 'n_estimators': 1000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Define base estimator for Bagging Regressor\n",
        "base_estimator = DecisionTreeRegressor(max_depth=5)\n",
        "\n",
        "# Train the Bagging Regressor model\n",
        "bagging_model = BaggingRegressor(base_estimator=base_estimator, n_estimators=100, random_state=1)\n",
        "bagging_model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions using Bagging Regressor\n",
        "Y_pred_bagging = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Bagging Regressor model\n",
        "mse_bagging = mean_squared_error(Y_test, Y_pred_bagging)\n",
        "r2_bagging = r2_score(Y_test, Y_pred_bagging)\n",
        "\n",
        "print('Bagging Regressor Model Evaluation:')\n",
        "print('Mean Squared Error (MSE): ', mse_bagging)\n",
        "print('R-squared (R2) Score: ', r2_bagging)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o6oaWIfTaqb",
        "outputId": "880bd204-04a2-4797-ec13-ec456f2e87ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor Model Evaluation:\n",
            "Mean Squared Error (MSE):  354.4911761434549\n",
            "R-squared (R2) Score:  0.8159638974892081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "\n",
        "# Train the ExtraTreesRegressor model\n",
        "extra_trees_model = ExtraTreesRegressor(random_state=1)\n",
        "extra_trees_model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions using ExtraTreesRegressor\n",
        "Y_pred_extra_trees = extra_trees_model.predict(X_test)\n",
        "\n",
        "# Evaluate the ExtraTreesRegressor model\n",
        "mse_extra_trees = mean_squared_error(Y_test, Y_pred_extra_trees)\n",
        "r2_extra_trees = r2_score(Y_test, Y_pred_extra_trees)\n",
        "\n",
        "print('ExtraTreesRegressor Model Evaluation:')\n",
        "print('Mean Squared Error (MSE): ', mse_extra_trees)\n",
        "print('R-squared (R2) Score: ', r2_extra_trees)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-qhq3uIUPMQ",
        "outputId": "d55989c2-0657-4dc2-f507-295d1920affc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExtraTreesRegressor Model Evaluation:\n",
            "Mean Squared Error (MSE):  331.37771385383803\n",
            "R-squared (R2) Score:  0.8279633823891929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Defining hyper-parameter optimization function\n",
        "def hyper_param_opt_model(X, y, model, param_grid):\n",
        "    \"\"\"\n",
        "    Hyper-parameter optimization by GridSearch\n",
        "    \"\"\"\n",
        "    gsc = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid,\n",
        "        cv=5,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        verbose=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_result = gsc.fit(X, y)\n",
        "    best_params = grid_result.best_params_\n",
        "    return best_params\n",
        "\n",
        "# Defining the parameter grid for RandomForestRegressor\n",
        "param_grid_rgr = {\n",
        "    'max_depth': range(11, 16),\n",
        "    'n_estimators': (5, 50, 100, 200, 1000)\n",
        "}\n",
        "\n",
        "# Finding the best parameters for RandomForestRegressor\n",
        "best_params_rgr = hyper_param_opt_model(X_train, Y_train, RandomForestRegressor(random_state=1), param_grid_rgr)\n",
        "print('The best parameters for RandomForestRegressor: ', best_params_rgr)\n"
      ],
      "metadata": {
        "id": "RxVb1UnXUmZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6f1051-e7fa-4690-d173-18b46ebdd252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best parameters for RandomForestRegressor:  {'max_depth': 15, 'n_estimators': 1000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_rgr_model = RandomForestRegressor(random_state=1, **best_params_rgr)\n",
        "best_rgr_model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions\n",
        "Y_pred = best_rgr_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(Y_test, Y_pred)\n",
        "r2 = r2_score(Y_test, Y_pred)\n",
        "\n",
        "print('Mean Squared Error (MSE): ', mse)\n",
        "print('R-squared (R2) Score: ', r2)"
      ],
      "metadata": {
        "id": "UXTEN4NFUQvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94787b12-a348-4bca-b48d-62ec77c73763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE):  245.4510024706385\n",
            "R-squared (R2) Score:  0.872572721432753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install catboost\n",
        "\n",
        "# # from catboost import CatBoostRegressor\n",
        "# # from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # # Define the parameter grid for CatBoostRegressor\n",
        "# # param_grid_catboost = {\n",
        "# #     'learning_rate': [0.01, 0.05, 0.1],\n",
        "# #     'depth': [3, 5, 7],\n",
        "# #     'l2_leaf_reg': [1, 3, 5]\n",
        "# # }\n",
        "\n",
        "# # # Instantiate the CatBoostRegressor model\n",
        "# # catboost_model = CatBoostRegressor(iterations=100, verbose=0, random_state=1)\n",
        "\n",
        "# # # Instantiate GridSearchCV for CatBoostRegressor\n",
        "# # grid_search_catboost = GridSearchCV(estimator=catboost_model, param_grid=param_grid_catboost, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "\n",
        "# # # Fit the model\n",
        "# # grid_search_catboost.fit(X_train, Y_train)\n",
        "\n",
        "# # # Get the best parameters and the best estimator for CatBoostRegressor\n",
        "# # best_params_catboost = grid_search_catboost.best_params_\n",
        "# # best_catboost_model = grid_search_catboost.best_estimator_\n",
        "\n",
        "# # # Make predictions using the best CatBoostRegressor model\n",
        "# # Y_pred_catboost = best_catboost_model.predict(X_test)\n",
        "\n",
        "# # # Evaluate the best CatBoostRegressor model\n",
        "# # mse_catboost = mean_squared_error(Y_test, Y_pred_catboost)\n",
        "# # r2_catboost = r2_score(Y_test, Y_pred_catboost)\n",
        "\n",
        "# # print('CatBoostRegressor Model Evaluation:')\n",
        "# # print('Mean Squared Error (MSE): ', mse_catboost)\n",
        "# # print('R-squared (R2) Score: ', r2_catboost)\n"
      ],
      "metadata": {
        "id": "Iq4J8ChIU8rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "# Define a grid of feature values for each feature (assuming X has n features)\n",
        "feature_grid = [np.linspace(min_value, max_value, num_points) for min_value, max_value, num_points in zip(X.min(axis=0), X.max(axis=0), [10]*n_features)]\n",
        "\n",
        "# Generate all possible combinations of feature values\n",
        "feature_combinations = product(*feature_grid)\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Iterate through each combination of feature values\n",
        "for features in feature_combinations:\n",
        "    # Predict the target variable using the CatBoostRegressor model\n",
        "    predicted_value = catboost_model.predict([features])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features\n",
        "\n",
        "print('Optimal feature values:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "MI5J0fsHc34B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "255b9257-df83-4450-9717-be402228a6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'n_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3cae25f35992>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define a grid of feature values for each feature (assuming X has n features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfeature_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_points\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_points\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Generate all possible combinations of feature values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for LightGBMRegressor\n",
        "param_grid_lgbm = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'num_leaves': [15, 31, 63]\n",
        "}\n",
        "\n",
        "# Instantiate the LGBMRegressor model\n",
        "lgbm_model = LGBMRegressor(random_state=1)\n",
        "\n",
        "# Instantiate GridSearchCV for LGBMRegressor\n",
        "grid_search_lgbm = GridSearchCV(estimator=lgbm_model, param_grid=param_grid_lgbm, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model\n",
        "grid_search_lgbm.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best parameters and the best estimator for LGBMRegressor\n",
        "best_params_lgbm = grid_search_lgbm.best_params_\n",
        "best_lgbm_model = grid_search_lgbm.best_estimator_\n",
        "\n",
        "# Make predictions using the best LGBMRegressor model\n",
        "Y_pred_lgbm = best_lgbm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best LGBMRegressor model\n",
        "mse_lgbm = mean_squared_error(Y_test, Y_pred_lgbm)\n",
        "r2_lgbm = r2_score(Y_test, Y_pred_lgbm)\n",
        "\n",
        "print('LightGBMRegressor Model Evaluation:')\n",
        "print('Mean Squared Error (MSE): ', mse_lgbm)\n",
        "print('R-squared (R2) Score: ', r2_lgbm)\n"
      ],
      "metadata": {
        "id": "S1AiRbguZaSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth' : [5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Instantiate the RandomForestRegressor model\n",
        "rf_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best parameters and the best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best RandomForestRegressor model\n",
        "Y_pred_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best RandomForestRegressor model\n",
        "mse_rf = mean_squared_error(Y_test, Y_pred_rf)\n",
        "r2_rf = r2_score(Y_test, Y_pred_rf)\n",
        "\n",
        "print('RandomForestRegressor Model Evaluation:')\n",
        "print('Mean Squared Error (MSE): ', mse_rf)\n",
        "print('R-squared (R2) Score: ', r2_rf)\n"
      ],
      "metadata": {
        "id": "Qz1lSEE_ZnMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "id": "8WSyP2SJZpvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f6b9c1-5d0f-4d7b-c6f9-8d57e5f50cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.3-cp310-cp310-manylinux2014_x86_64.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "# Train the CatBoostRegressor model\n",
        "catboost_model = CatBoostRegressor(random_state=1, verbose=0)\n",
        "catboost_model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions using CatBoostRegressor\n",
        "Y_pred_catboost = catboost_model.predict(X_test)\n",
        "\n",
        "# Evaluate the CatBoostRegressor model\n",
        "mse_catboost = mean_squared_error(Y_test, Y_pred_catboost)\n",
        "r2_catboost = r2_score(Y_test, Y_pred_catboost)\n",
        "\n",
        "print('\\nCatBoostRegressor Model Evaluation:')\n",
        "print('Mean Squared Error (MSE): ', mse_catboost)\n",
        "print('R-squared (R2) Score: ', r2_catboost)"
      ],
      "metadata": {
        "id": "eVwqAx9weMoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa0ce01-2538-43d3-d323-905c4ec28135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CatBoostRegressor Model Evaluation:\n",
            "Mean Squared Error (MSE):  219.40607205927685\n",
            "R-squared (R2) Score:  0.8860940946167559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "# Define a grid of feature values for each feature (assuming X has n features)\n",
        "feature_grid = [np.linspace(min_value, max_value, num_points) for min_value, max_value, num_points in zip(X.min(axis=0), X.max(axis=0), [10]*n_features)]\n",
        "\n",
        "# Generate all possible combinations of feature values\n",
        "feature_combinations = product(*feature_grid)\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Iterate through each combination of feature values\n",
        "for features in feature_combinations:\n",
        "    # Predict the target variable using the CatBoostRegressor model\n",
        "    predicted_value = catboost_model.predict([features])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features\n",
        "\n",
        "print('Optimal feature values:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "fM0jS6PWejsD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "460f8a6c-49dd-42fb-b709-ff0880faec26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'n_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-3cae25f35992>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define a grid of feature values for each feature (assuming X has n features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfeature_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_points\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_points\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Generate all possible combinations of feature values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve feature importance scores from the trained CatBoostRegressor model\n",
        "feature_importance = catboost_model.feature_importances_\n",
        "\n",
        "# Visualize feature importance scores (optional)\n",
        "# You can use matplotlib or seaborn for visualization\n",
        "\n",
        "# Identify the most important features based on feature importance scores\n",
        "most_important_features = np.argsort(feature_importance)[::-1][:k]  # Choose top 'k' most important features\n",
        "\n",
        "# Define a grid of feature values for the most important features\n",
        "feature_grid_most_important = [np.linspace(min_value, max_value, num_points) for min_value, max_value, num_points in zip(X[:, most_important_features].min(axis=0), X[:, most_important_features].max(axis=0), [10]*len(most_important_features))]\n",
        "\n",
        "# Generate all possible combinations of feature values for the most important features\n",
        "feature_combinations_most_important = product(*feature_grid_most_important)\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Iterate through each combination of feature values for the most important features\n",
        "for features_most_important in feature_combinations_most_important:\n",
        "    # Create a copy of the original feature values and update the most important feature values\n",
        "    features = X[0].copy()\n",
        "    features[most_important_features] = features_most_important\n",
        "\n",
        "    # Predict the target variable using the CatBoostRegressor model\n",
        "    predicted_value = catboost_model.predict([features])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "g9Nhc8_we69Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of top features to consider\n",
        "k = 5  # Adjust this value based on your preference\n",
        "\n",
        "# Identify the most important features based on feature importance scores\n",
        "most_important_features = np.argsort(feature_importance)[::-1][:k]\n",
        "\n",
        "# Define a grid of feature values for the most important features\n",
        "feature_grid_most_important = [np.linspace(min_value, max_value, num_points) for min_value, max_value, num_points in zip(X[:, most_important_features].min(axis=0), X[:, most_important_features].max(axis=0), [10]*len(most_important_features))]\n",
        "\n",
        "# Generate all possible combinations of feature values for the most important features\n",
        "feature_combinations_most_important = product(*feature_grid_most_important)\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Iterate through each combination of feature values for the most important features\n",
        "for features_most_important in feature_combinations_most_important:\n",
        "    # Create a copy of the original feature values and update the most important feature values\n",
        "    features = X[0].copy()\n",
        "    features[most_important_features] = features_most_important\n",
        "\n",
        "    # Predict the target variable using the CatBoostRegressor model\n",
        "    predicted_value = catboost_model.predict([features])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "0ynoppujfNw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a grid of feature values for all features\n",
        "feature_grid_all = [np.linspace(min_value, max_value, num_points) for min_value, max_value, num_points in zip(X.min(axis=0), X.max(axis=0), [10]*X.shape[1])]\n",
        "\n",
        "# Generate all possible combinations of feature values for all features\n",
        "feature_combinations_all = product(*feature_grid_all)\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Iterate through each combination of feature values for all features\n",
        "for features_all in feature_combinations_all:\n",
        "    # Predict the target variable using the CatBoostRegressor model\n",
        "    predicted_value = catboost_model.predict([features_all])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_all\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "J_iNzNA5fhwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 1000\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Iterate through each sample\n",
        "for _ in range(num_samples):\n",
        "    # Generate random feature values\n",
        "    random_features = [random.uniform(min_value, max_value) for min_value, max_value in zip(X.min(axis=0), X.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the CatBoostRegressor model\n",
        "    predicted_value = catboost_model.predict([random_features])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = random_features\n",
        "\n",
        "# Retrieve feature names (column titles) from the dataset\n",
        "feature_names = dataset.columns.tolist()\n",
        "\n",
        "# Create a dictionary to store feature names and their corresponding values\n",
        "feature_names_values = {feature_name: feature_value for feature_name, feature_value in zip(feature_names, optimal_feature_values)}\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:')\n",
        "for feature_name, feature_value in feature_names_values.items():\n",
        "    print(feature_name + ':', feature_value)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "bz9g1qw2gFSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5703029-fc0a-467f-ff4b-7cc53d54b98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for highest predicted Y value:\n",
            "Al: 78.87271669062264\n",
            "Fe: 0.06953463149706618\n",
            "Si: 0.042013689305253246\n",
            "Zn: 2.0019512497367478\n",
            "Cu: 1.0203755496209859\n",
            "V: 0.03143784525357092\n",
            "Zr: 0.03237143867770236\n",
            "Ag: 0.26032161889845584\n",
            "Mn: 0.12508027755874992\n",
            "Ni: 0.8515624392066721\n",
            "Sn: 0.947714154980419\n",
            "Be: 0.11068670960124401\n",
            "Ti: 0.022880042986721666\n",
            "Mg: 2.804814277559492\n",
            "B: 0.018281773050379754\n",
            "Ga: 0.02078494884777217\n",
            "Cr: 0.04914379908719274\n",
            "Pb: 0.06320873820564676\n",
            "Bi: 0.04982570470181524\n",
            "Li: 0.07896077934013257\n",
            "Co: 0.0009600861261754496\n",
            "Fatigue Strength: 74.59202779998523\n",
            "Predicted highest Y value: [160.45015977]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "l6fBB7J6fUht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of random samples to generate\n",
        "num_samples = 100000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X.min(axis=0), X.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the CatBoostRegressor model\n",
        "    predicted_value = catboost_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "YKegwr-UgXDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8ada5f-2128-452e-b1e4-d4a6f3351abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for highest predicted Y value: [78.48678080109418, 0.2098362981111231, 0.22412939421904013, 2.6406663443783147, 7.065739859699131, 0.09074208978799701, 0.18612851873782, 0.1424673328154528, 0.42222769240333125, 0.2490183665259446, 1.132033979330438, 0.1532310517527692, 0.010419198552194808, 3.5162881981889016, 0.029677403105913082, 0.019290168513244818, 0.10941844318648863, 0.04531399232938575, 0.6352407333505401, 0.7239714507850129, 0.005628460917412173, 84.07947149156566]\n",
            "Predicted highest Y value: [157.7080181]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 1000000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)"
      ],
      "metadata": {
        "id": "Zm4QPDHVg7pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rfr_model, param_distributions=param_grid, n_iter=100, cv=5, scoring='neg_mean_squared_error', random_state=1, n_jobs=-1)\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "random_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best RandomForestRegressor model\n",
        "best_rfr_model = random_search.best_estimator_\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 1000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the best RandomForestRegressor model\n",
        "    predicted_value = best_rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "4qJ1g3i5hUAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of iterations for refinement\n",
        "num_iterations = 100000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Start with random feature values\n",
        "current_feature_values = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "# Iterate to refine feature values towards maximum predicted Y value\n",
        "for _ in range(num_iterations):\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([current_feature_values])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = current_feature_values.copy()\n",
        "\n",
        "    # Randomly perturb one feature value\n",
        "    index_to_perturb = np.random.randint(len(current_feature_values))\n",
        "    current_feature_values[index_to_perturb] = np.random.uniform(X_train.min(axis=0)[index_to_perturb], X_train.max(axis=0)[index_to_perturb])\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "QDVvD8BkmfSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the learning rate for gradient ascent\n",
        "learning_rate = 0.01  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize feature values randomly\n",
        "current_feature_values = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "# Define the number of iterations for gradient ascent\n",
        "num_iterations = 1000  # Adjust this value based on your preference\n",
        "\n",
        "# Iterate to optimize feature values using gradient ascent\n",
        "for _ in range(num_iterations):\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([current_feature_values])[0]\n",
        "\n",
        "    # Compute the gradient of the predicted Y value with respect to the feature values\n",
        "    gradient = np.zeros(len(current_feature_values))\n",
        "    for i in range(len(current_feature_values)):\n",
        "        # Perturb the feature value\n",
        "        perturbed_feature_values = current_feature_values.copy()\n",
        "        perturbed_feature_values[i] += 0.0001  # Small perturbation\n",
        "\n",
        "        # Compute the perturbed predicted Y value\n",
        "        perturbed_predicted_value = rfr_model.predict([perturbed_feature_values])[0]\n",
        "\n",
        "        # Compute the gradient using finite differences\n",
        "        gradient[i] = (perturbed_predicted_value - predicted_value) / 0.0001\n",
        "\n",
        "    # Update feature values using gradient ascent\n",
        "    current_feature_values += learning_rate * gradient\n",
        "\n",
        "# Predict the target variable using the final feature values\n",
        "predicted_value = rfr_model.predict([current_feature_values])[0]\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', current_feature_values)\n",
        "print('Predicted highest Y value:', predicted_value)\n"
      ],
      "metadata": {
        "id": "aJbmm-86oLyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the learning rate for gradient ascent\n",
        "learning_rate = 0.01  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize feature values randomly\n",
        "current_feature_values = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "# Define the number of iterations for gradient ascent\n",
        "num_iterations = 1000  # Adjust this value based on your preference\n",
        "\n",
        "# Iterate to optimize feature values using gradient ascent\n",
        "for _ in range(num_iterations):\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([current_feature_values])[0]\n",
        "\n",
        "    # Compute the gradient of the predicted Y value with respect to the feature values\n",
        "    gradient = np.zeros(len(current_feature_values))\n",
        "    for i in range(len(current_feature_values)):\n",
        "        # Perturb the feature value\n",
        "        perturbed_feature_values = current_feature_values.copy()\n",
        "        perturbed_feature_values[i] += 0.0001  # Small perturbation\n",
        "\n",
        "        # Compute the perturbed predicted Y value\n",
        "        perturbed_predicted_value = rfr_model.predict([perturbed_feature_values])[0]\n",
        "\n",
        "        # Compute the gradient using finite differences\n",
        "        gradient[i] = (perturbed_predicted_value - predicted_value) / 0.0001\n",
        "\n",
        "    # Update feature values using gradient ascent\n",
        "    current_feature_values += learning_rate * gradient\n",
        "\n",
        "# Predict the target variable using the final feature values\n",
        "predicted_value = rfr_model.predict([current_feature_values])[0]\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', current_feature_values)\n",
        "print('Predicted highest Y value:', predicted_value)\n"
      ],
      "metadata": {
        "id": "aMMsEpqnwsk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 100000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)"
      ],
      "metadata": {
        "id": "qNnOFZ98LudC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15101047-ea90-40b2-bbb2-d1983272d48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for highest predicted Y value: [89.43060679934018, 0.7494843837198794, 0.1397695419783953, 1.5896496186600395, 0.6507715210363575, 0.08856358739566314, 0.18238332190705897, 0.2869716435308236, 1.085815575555214, 1.2162250863494404, 3.588205567218291, 0.11119293395955179, 0.027503815937186593, 5.326782513089788, 0.008531116367407665, 0.015741001083844824, 0.1644734618515073, 0.3472483482772754, 0.7673350141417493, 0.4355806556130649, 0.005609150657893917, 18.048065179993266]\n",
            "Predicted highest Y value: [200.03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=2)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 10000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)"
      ],
      "metadata": {
        "id": "H_OhdDJ9M2ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99af290-6c1a-4187-a8f8-b47a7142daf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for highest predicted Y value: [84.93667436550683, 0.30938171127030684, 0.052972264832087335, 7.731520131841515, 5.010745072099786, 0.10560368766869152, 0.01016799124692025, 0.40272757869259157, 0.6234480476445756, 2.2359369541048837, 2.842993778560405, 0.15509229104851385, 0.012837240601386773, 6.161256155170042, 0.005963500917715316, 0.0009158201218520817, 0.20335205870639428, 0.9677134496114168, 0.4941540169122949, 0.07214211858149744, 0.009933516638317031, 54.84176531505142]\n",
            "Predicted highest Y value: [199.06]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target Y value\n",
        "target_y = 215\n",
        "\n",
        "# Define the learning rate for gradient ascent\n",
        "learning_rate = 0.01  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize feature values randomly\n",
        "current_feature_values = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "# Define the maximum number of iterations\n",
        "max_iterations = 10000\n",
        "\n",
        "# Define a tolerance level for the target Y value\n",
        "tolerance = 0.1  # Adjust this value based on your preference\n",
        "\n",
        "# Iterate to find feature values for the target Y value\n",
        "for iteration in range(max_iterations):\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([current_feature_values])[0]\n",
        "\n",
        "    # Check if the predicted Y value is close to the target Y value\n",
        "    if abs(predicted_value - target_y) < tolerance:\n",
        "        break  # Stop iteration if the predicted Y value is within the tolerance level of the target Y value\n",
        "\n",
        "    # Compute the gradient of the predicted Y value with respect to the feature values\n",
        "    gradient = np.zeros(len(current_feature_values))\n",
        "    for i in range(len(current_feature_values)):\n",
        "        # Perturb the feature value\n",
        "        perturbed_feature_values = current_feature_values.copy()\n",
        "        perturbed_feature_values[i] += 0.0001  # Small perturbation\n",
        "\n",
        "        # Compute the perturbed predicted Y value\n",
        "        perturbed_predicted_value = rfr_model.predict([perturbed_feature_values])[0]\n",
        "\n",
        "        # Compute the gradient using finite differences\n",
        "        gradient[i] = (perturbed_predicted_value - predicted_value) / 0.0001\n",
        "\n",
        "    # Update feature values using gradient ascent\n",
        "    current_feature_values += learning_rate * gradient\n",
        "\n",
        "# Predict the target variable using the final feature values\n",
        "predicted_value = rfr_model.predict([current_feature_values])[0]\n",
        "\n",
        "print('Optimal feature values for Y = 215:', current_feature_values)\n",
        "print('Predicted Y value:', predicted_value)\n"
      ],
      "metadata": {
        "id": "F1fypWthM8xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 1000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)"
      ],
      "metadata": {
        "id": "tLcemoSsPq6s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c3fc41f1-0fe4-4e3c-a57a-2810cfc22b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f4017e417212>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Fit the RandomForestRegressor model with the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrfr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define the number of random samples to generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 10000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9ycoBJATAwo",
        "outputId": "dbd9eeb1-a641-4985-c22f-e87979f448da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for highest predicted Y value: [93.06532219319618, 1.1358368423191556, 0.07166277990321451, 0.5787168512552684, 2.137470874650111, 0.07782550963323397, 0.15447277996536837, 0.6245316194705949, 1.1647191965482446, 0.7553097117160251, 4.430129884786545, 0.11436197038202107, 0.07946567894255721, 9.126822201587755, 0.004091834339674928, 0.015305181554451902, 0.3996683787950368, 1.115531755345818, 0.11815303919348974, 0.8679860803506013, 0.002620628075767753, 19.737372631195527]\n",
            "Predicted highest Y value: [198.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 100000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSrBC3QHTAaT",
        "outputId": "75562356-7002-43b0-8012-8582109d957d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for highest predicted Y value: [84.83658367694764, 0.09953181783766127, 0.20243126142980003, 6.893031583059888, 4.5386687335783975, 0.0383451060751409, 0.04342996991034294, 0.09016071539810683, 0.3440765489926781, 0.030727959099520752, 0.9910473697604547, 0.09214902281492886, 0.20482838220776278, 8.250944681546214, 0.00620355571535783, 0.0011432501212209456, 0.20129243522381646, 0.07932038440415345, 0.31642409574498964, 0.9441230739835622, 0.02419591915815572, 78.34412194948513]\n",
            "Predicted highest Y value: [199.3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=2)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 1000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo1YpNKETGiV",
        "outputId": "a86aa2a2-5baf-47e5-c74b-648671c5d9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for highest predicted Y value: [85.28911401241461, 0.0903886293183573, 0.3739976247269026, 6.819302365786251, 2.801894822796146, 0.12293001322965053, 0.17724724334754094, 0.27244926312475215, 0.05485976649426516, 0.5713177346710943, 2.4335831653937996, 0.1927233149532187, 0.23622742738346802, 3.7111338187579195, 0.025587508179156965, 0.013876755355855322, 0.24065631339378613, 0.1948810165209074, 0.5033552618134874, 0.05900556665122125, 0.014991268112334494, 55.05115380289267]\n",
            "Predicted highest Y value: [197.4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=2)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 10000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm2L7jURTK-A",
        "outputId": "7cf153f9-4d32-433a-91da-1ff298625625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for highest predicted Y value: [83.0076262519024, 0.10706105737198342, 0.07750317720678176, 6.461781227874871, 7.394061973310915, 0.09250475032599242, 0.16513273021186267, 0.5003780750696473, 0.36812103599743656, 0.5612214766371684, 0.854494579242529, 0.15157192343155457, 0.06049037314113048, 9.203830205647506, 0.001133201494140691, 0.021656362307989386, 0.34887257955831097, 0.2509865121584474, 0.9688372667081347, 0.2666711793276705, 0.0007572729715136562, 55.51304604471094]\n",
            "Predicted highest Y value: [197.8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 100000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)\n"
      ],
      "metadata": {
        "id": "tGkjHlIVQGgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bayesian-optimization\n"
      ],
      "metadata": {
        "id": "iiFysMx5yYaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b3f7be-9c5e-497d-8076-c96f49b90d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.4.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Define the objective function to maximize (in this case, the negative mean squared error)\n",
        "def rf_objective_function(learning_rate, n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=int(n_estimators),\n",
        "        max_depth=int(max_depth),\n",
        "        min_samples_split=int(min_samples_split),\n",
        "        min_samples_leaf=int(min_samples_leaf),\n",
        "        max_features=max_features,\n",
        "        random_state=1\n",
        "    )\n",
        "    model.fit(X_train, Y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = -1 * np.mean((Y_test - y_pred) ** 2)  # Negative mean squared error (to maximize)\n",
        "    return mse\n",
        "\n",
        "# Define the parameter space for Bayesian optimization\n",
        "rf_parameter_space = {\n",
        "    'learning_rate': (0.001, 0.1),\n",
        "    'n_estimators': (10, 200),\n",
        "    'max_depth': (3, 20),\n",
        "    'min_samples_split': (2, 20),\n",
        "    'min_samples_leaf': (1, 10),\n",
        "    'max_features': (0.1, 1.0)\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "rf_optimizer = BayesianOptimization(f=rf_objective_function, pbounds=rf_parameter_space, random_state=1)\n",
        "rf_optimizer.maximize(init_points=5, n_iter=50)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_rf_params = rf_optimizer.max['params']\n",
        "\n",
        "print('Best hyperparameters for RandomForestRegressor:')\n",
        "print(best_rf_params)\n"
      ],
      "metadata": {
        "id": "YDN-sRtzcY11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f35b2b18-54ad-49f7-991c-c3d59986c697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | max_fe... | min_sa... | min_sa... | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-467.2   \u001b[0m | \u001b[0m0.04229  \u001b[0m | \u001b[0m15.25    \u001b[0m | \u001b[0m0.1001   \u001b[0m | \u001b[0m3.721    \u001b[0m | \u001b[0m4.642    \u001b[0m | \u001b[0m27.54    \u001b[0m |\n",
            "| \u001b[95m2        \u001b[0m | \u001b[95m-343.9   \u001b[0m | \u001b[95m0.01944  \u001b[0m | \u001b[95m8.875    \u001b[0m | \u001b[95m0.4571   \u001b[0m | \u001b[95m5.849    \u001b[0m | \u001b[95m9.546    \u001b[0m | \u001b[95m140.2    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m-584.3   \u001b[0m | \u001b[0m0.02124  \u001b[0m | \u001b[0m17.93    \u001b[0m | \u001b[0m0.1246   \u001b[0m | \u001b[0m7.034    \u001b[0m | \u001b[0m9.511    \u001b[0m | \u001b[0m116.2    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m-382.1   \u001b[0m | \u001b[0m0.0149   \u001b[0m | \u001b[0m6.368    \u001b[0m | \u001b[0m0.8207   \u001b[0m | \u001b[0m9.714    \u001b[0m | \u001b[0m7.642    \u001b[0m | \u001b[0m141.5    \u001b[0m |\n",
            "| \u001b[95m5        \u001b[0m | \u001b[95m-321.5   \u001b[0m | \u001b[95m0.08776  \u001b[0m | \u001b[95m18.21    \u001b[0m | \u001b[95m0.1765   \u001b[0m | \u001b[95m1.351    \u001b[0m | \u001b[95m5.057    \u001b[0m | \u001b[95m176.8    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m-327.8   \u001b[0m | \u001b[0m0.06442  \u001b[0m | \u001b[0m15.83    \u001b[0m | \u001b[0m0.1072   \u001b[0m | \u001b[0m1.123    \u001b[0m | \u001b[0m7.672    \u001b[0m | \u001b[0m162.8    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-462.4   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m7.457    \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m174.6    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m-369.8   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m168.3    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m-358.3   \u001b[0m | \u001b[0m0.008664 \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m11.99    \u001b[0m | \u001b[0m148.8    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m-341.7   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m192.1    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m-814.0   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m145.9    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m-347.2   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m16.61    \u001b[0m | \u001b[0m0.3604   \u001b[0m | \u001b[0m4.987    \u001b[0m | \u001b[0m2.579    \u001b[0m | \u001b[0m142.6    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m-370.7   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m7.858    \u001b[0m | \u001b[0m184.8    \u001b[0m |\n",
            "| \u001b[95m14       \u001b[0m | \u001b[95m-262.0   \u001b[0m | \u001b[95m0.001    \u001b[0m | \u001b[95m8.22     \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m184.6    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m-658.9   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m4.96     \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m173.7    \u001b[0m |\n",
            "| \u001b[95m16       \u001b[0m | \u001b[95m-237.2   \u001b[0m | \u001b[95m0.04872  \u001b[0m | \u001b[95m14.49    \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m1.573    \u001b[0m | \u001b[95m3.698    \u001b[0m | \u001b[95m185.3    \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m-261.7   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m8.83     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m6.2      \u001b[0m | \u001b[0m192.1    \u001b[0m |\n",
            "| \u001b[0m18       \u001b[0m | \u001b[0m-370.4   \u001b[0m | \u001b[0m0.04023  \u001b[0m | \u001b[0m9.872    \u001b[0m | \u001b[0m0.7781   \u001b[0m | \u001b[0m9.24     \u001b[0m | \u001b[0m2.082    \u001b[0m | \u001b[0m190.1    \u001b[0m |\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0m-313.5   \u001b[0m | \u001b[0m0.04456  \u001b[0m | \u001b[0m17.26    \u001b[0m | \u001b[0m0.3778   \u001b[0m | \u001b[0m1.12     \u001b[0m | \u001b[0m15.13    \u001b[0m | \u001b[0m190.4    \u001b[0m |\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0m-438.0   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m4.598    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m17.59    \u001b[0m | \u001b[0m197.9    \u001b[0m |\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0m-369.9   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m15.03    \u001b[0m | \u001b[0m161.4    \u001b[0m |\n",
            "| \u001b[0m22       \u001b[0m | \u001b[0m-272.4   \u001b[0m | \u001b[0m0.07182  \u001b[0m | \u001b[0m10.18    \u001b[0m | \u001b[0m0.6601   \u001b[0m | \u001b[0m1.487    \u001b[0m | \u001b[0m10.69    \u001b[0m | \u001b[0m186.3    \u001b[0m |\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0m-559.6   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m68.78    \u001b[0m |\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0m-510.9   \u001b[0m | \u001b[0m0.07341  \u001b[0m | \u001b[0m4.484    \u001b[0m | \u001b[0m0.3693   \u001b[0m | \u001b[0m2.877    \u001b[0m | \u001b[0m19.38    \u001b[0m | \u001b[0m10.01    \u001b[0m |\n",
            "| \u001b[0m25       \u001b[0m | \u001b[0m-616.7   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m154.5    \u001b[0m |\n",
            "| \u001b[0m26       \u001b[0m | \u001b[0m-251.5   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m9.947    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m134.0    \u001b[0m |\n",
            "| \u001b[0m27       \u001b[0m | \u001b[0m-369.1   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m11.97    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m132.4    \u001b[0m |\n",
            "| \u001b[0m28       \u001b[0m | \u001b[0m-237.2   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m18.43    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m7.91     \u001b[0m | \u001b[0m134.7    \u001b[0m |\n",
            "| \u001b[0m29       \u001b[0m | \u001b[0m-381.8   \u001b[0m | \u001b[0m0.05957  \u001b[0m | \u001b[0m19.81    \u001b[0m | \u001b[0m0.2134   \u001b[0m | \u001b[0m3.775    \u001b[0m | \u001b[0m17.22    \u001b[0m | \u001b[0m135.3    \u001b[0m |\n",
            "| \u001b[0m30       \u001b[0m | \u001b[0m-267.5   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m15.64    \u001b[0m | \u001b[0m169.9    \u001b[0m |\n",
            "| \u001b[0m31       \u001b[0m | \u001b[0m-370.3   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m176.2    \u001b[0m |\n",
            "| \u001b[0m32       \u001b[0m | \u001b[0m-240.8   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m82.86    \u001b[0m |\n",
            "| \u001b[0m33       \u001b[0m | \u001b[0m-346.7   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m71.72    \u001b[0m |\n",
            "| \u001b[0m34       \u001b[0m | \u001b[0m-253.1   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m9.983    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m87.73    \u001b[0m |\n",
            "| \u001b[0m35       \u001b[0m | \u001b[0m-363.5   \u001b[0m | \u001b[0m0.08688  \u001b[0m | \u001b[0m18.22    \u001b[0m | \u001b[0m0.9146   \u001b[0m | \u001b[0m9.792    \u001b[0m | \u001b[0m3.379    \u001b[0m | \u001b[0m89.18    \u001b[0m |\n",
            "| \u001b[0m36       \u001b[0m | \u001b[0m-254.9   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m16.12    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m12.27    \u001b[0m | \u001b[0m84.66    \u001b[0m |\n",
            "| \u001b[0m37       \u001b[0m | \u001b[0m-558.2   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m3.545    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m12.92    \u001b[0m | \u001b[0m91.75    \u001b[0m |\n",
            "| \u001b[0m38       \u001b[0m | \u001b[0m-242.3   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m10.11    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m78.61    \u001b[0m |\n",
            "| \u001b[0m39       \u001b[0m | \u001b[0m-271.9   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m16.89    \u001b[0m | \u001b[0m75.98    \u001b[0m |\n",
            "| \u001b[0m40       \u001b[0m | \u001b[0m-351.0   \u001b[0m | \u001b[0m0.05675  \u001b[0m | \u001b[0m19.23    \u001b[0m | \u001b[0m0.8397   \u001b[0m | \u001b[0m8.349    \u001b[0m | \u001b[0m8.773    \u001b[0m | \u001b[0m77.87    \u001b[0m |\n",
            "| \u001b[0m41       \u001b[0m | \u001b[0m-389.5   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m87.21    \u001b[0m |\n",
            "| \u001b[0m42       \u001b[0m | \u001b[0m-291.9   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m52.49    \u001b[0m |\n",
            "| \u001b[0m43       \u001b[0m | \u001b[0m-237.9   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.759    \u001b[0m | \u001b[0m51.47    \u001b[0m |\n",
            "| \u001b[0m44       \u001b[0m | \u001b[0m-373.7   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.61    \u001b[0m | \u001b[0m48.17    \u001b[0m |\n",
            "| \u001b[0m45       \u001b[0m | \u001b[0m-247.6   \u001b[0m | \u001b[0m0.04742  \u001b[0m | \u001b[0m10.26    \u001b[0m | \u001b[0m0.9376   \u001b[0m | \u001b[0m1.035    \u001b[0m | \u001b[0m3.205    \u001b[0m | \u001b[0m55.46    \u001b[0m |\n",
            "| \u001b[0m46       \u001b[0m | \u001b[0m-265.8   \u001b[0m | \u001b[0m0.09358  \u001b[0m | \u001b[0m16.67    \u001b[0m | \u001b[0m0.9062   \u001b[0m | \u001b[0m1.22     \u001b[0m | \u001b[0m12.04    \u001b[0m | \u001b[0m61.79    \u001b[0m |\n",
            "| \u001b[0m47       \u001b[0m | \u001b[0m-288.3   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m7.684    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m12.42    \u001b[0m | \u001b[0m48.48    \u001b[0m |\n",
            "| \u001b[0m48       \u001b[0m | \u001b[0m-504.6   \u001b[0m | \u001b[0m0.08407  \u001b[0m | \u001b[0m4.326    \u001b[0m | \u001b[0m0.2944   \u001b[0m | \u001b[0m2.672    \u001b[0m | \u001b[0m2.143    \u001b[0m | \u001b[0m46.64    \u001b[0m |\n",
            "| \u001b[0m49       \u001b[0m | \u001b[0m-376.4   \u001b[0m | \u001b[0m0.006824 \u001b[0m | \u001b[0m15.74    \u001b[0m | \u001b[0m0.5474   \u001b[0m | \u001b[0m8.255    \u001b[0m | \u001b[0m2.575    \u001b[0m | \u001b[0m57.93    \u001b[0m |\n",
            "| \u001b[0m50       \u001b[0m | \u001b[0m-357.0   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m14.08    \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m11.16    \u001b[0m | \u001b[0m53.95    \u001b[0m |\n",
            "| \u001b[0m51       \u001b[0m | \u001b[0m-390.1   \u001b[0m | \u001b[0m0.07843  \u001b[0m | \u001b[0m6.593    \u001b[0m | \u001b[0m0.2715   \u001b[0m | \u001b[0m1.277    \u001b[0m | \u001b[0m3.663    \u001b[0m | \u001b[0m63.43    \u001b[0m |\n",
            "| \u001b[0m52       \u001b[0m | \u001b[0m-346.2   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m14.25    \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m6.115    \u001b[0m | \u001b[0m81.89    \u001b[0m |\n",
            "| \u001b[0m53       \u001b[0m | \u001b[0m-241.6   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m6.454    \u001b[0m | \u001b[0m89.65    \u001b[0m |\n",
            "| \u001b[0m54       \u001b[0m | \u001b[0m-286.0   \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m19.44    \u001b[0m | \u001b[0m65.41    \u001b[0m |\n",
            "| \u001b[0m55       \u001b[0m | \u001b[0m-558.2   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m80.7     \u001b[0m |\n",
            "=================================================================================================\n",
            "Best hyperparameters for RandomForestRegressor:\n",
            "{'learning_rate': 0.04872326842015106, 'max_depth': 14.489408614996492, 'max_features': 1.0, 'min_samples_leaf': 1.572755167443494, 'min_samples_split': 3.6983178912802175, 'n_estimators': 185.3272108108215}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Define the objective function to maximize (in this case, the negative mean squared error)\n",
        "def catboost_objective_function(learning_rate, depth, bagging_temperature, l2_leaf_reg):\n",
        "    model = CatBoostRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        depth=int(depth),\n",
        "        bagging_temperature=bagging_temperature,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        verbose=0,\n",
        "        random_seed=1\n",
        "    )\n",
        "    model.fit(X_train, Y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = -1 * np.mean((Y_test - y_pred) ** 2)  # Negative mean squared error (to maximize)\n",
        "    return mse\n",
        "\n",
        "# Define the parameter space for Bayesian optimization\n",
        "catboost_parameter_space = {\n",
        "    'learning_rate': (0.001, 0.1),\n",
        "    'depth': (3, 10),\n",
        "    'bagging_temperature': (0.0, 10.0),\n",
        "    'l2_leaf_reg': (0.0, 5.0)\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "catboost_optimizer = BayesianOptimization(f=catboost_objective_function, pbounds=catboost_parameter_space, random_state=1)\n",
        "catboost_optimizer.maximize(init_points=5, n_iter=50)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_catboost_params = catboost_optimizer.max['params']\n",
        "\n",
        "print('Best hyperparameters for CatBoostRegressor:')\n",
        "print(best_catboost_params)\n"
      ],
      "metadata": {
        "id": "JFE3RBLddMFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65000a2e-07de-41ce-fe57-7427073808c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | baggin... |   depth   | l2_lea... | learni... |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-216.4   \u001b[0m | \u001b[0m4.17     \u001b[0m | \u001b[0m8.042    \u001b[0m | \u001b[0m0.0005719\u001b[0m | \u001b[0m0.03093  \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m-249.3   \u001b[0m | \u001b[0m1.468    \u001b[0m | \u001b[0m3.646    \u001b[0m | \u001b[0m0.9313   \u001b[0m | \u001b[0m0.03521  \u001b[0m |\n",
            "| \u001b[95m3        \u001b[0m | \u001b[95m-215.8   \u001b[0m | \u001b[95m3.968    \u001b[0m | \u001b[95m6.772    \u001b[0m | \u001b[95m2.096    \u001b[0m | \u001b[95m0.06884  \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m-226.7   \u001b[0m | \u001b[0m2.045    \u001b[0m | \u001b[0m9.147    \u001b[0m | \u001b[0m0.1369   \u001b[0m | \u001b[0m0.06738  \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m-219.4   \u001b[0m | \u001b[0m4.173    \u001b[0m | \u001b[0m6.911    \u001b[0m | \u001b[0m0.7019   \u001b[0m | \u001b[0m0.02061  \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m-216.1   \u001b[0m | \u001b[0m6.381    \u001b[0m | \u001b[0m9.011    \u001b[0m | \u001b[0m2.8      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-221.8   \u001b[0m | \u001b[0m6.833    \u001b[0m | \u001b[0m5.571    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m-796.3   \u001b[0m | \u001b[0m3.356    \u001b[0m | \u001b[0m9.148    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m0.001    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m-230.9   \u001b[0m | \u001b[0m8.373    \u001b[0m | \u001b[0m6.537    \u001b[0m | \u001b[0m1.47     \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m-224.3   \u001b[0m | \u001b[0m5.739    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m2.327    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m-240.6   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m2.828    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m-244.5   \u001b[0m | \u001b[0m7.496    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m-902.7   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m4.773    \u001b[0m | \u001b[0m0.001    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m-223.6   \u001b[0m | \u001b[0m3.117    \u001b[0m | \u001b[0m3.069    \u001b[0m | \u001b[0m4.992    \u001b[0m | \u001b[0m0.07613  \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m-227.0   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m6.644    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m-223.9   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m-226.8   \u001b[0m | \u001b[0m4.612    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m18       \u001b[0m | \u001b[0m-219.3   \u001b[0m | \u001b[0m9.126    \u001b[0m | \u001b[0m8.718    \u001b[0m | \u001b[0m4.964    \u001b[0m | \u001b[0m0.06943  \u001b[0m |\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0m-230.3   \u001b[0m | \u001b[0m5.994    \u001b[0m | \u001b[0m5.782    \u001b[0m | \u001b[0m2.476    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0m-692.2   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m8.215    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.001    \u001b[0m |\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0m-222.8   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m5.59     \u001b[0m | \u001b[0m3.148    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m22       \u001b[0m | \u001b[0m-226.8   \u001b[0m | \u001b[0m7.699    \u001b[0m | \u001b[0m3.467    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0m-244.5   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0m-848.8   \u001b[0m | \u001b[0m3.181    \u001b[0m | \u001b[0m4.365    \u001b[0m | \u001b[0m2.77     \u001b[0m | \u001b[0m0.001    \u001b[0m |\n",
            "| \u001b[0m25       \u001b[0m | \u001b[0m-226.7   \u001b[0m | \u001b[0m5.371    \u001b[0m | \u001b[0m7.559    \u001b[0m | \u001b[0m1.773    \u001b[0m | \u001b[0m0.0808   \u001b[0m |\n",
            "| \u001b[95m26       \u001b[0m | \u001b[95m-214.1   \u001b[0m | \u001b[95m7.283    \u001b[0m | \u001b[95m7.204    \u001b[0m | \u001b[95m3.22     \u001b[0m | \u001b[95m0.1      \u001b[0m |\n",
            "| \u001b[0m27       \u001b[0m | \u001b[0m-220.9   \u001b[0m | \u001b[0m6.55     \u001b[0m | \u001b[0m5.218    \u001b[0m | \u001b[0m0.4958   \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m28       \u001b[0m | \u001b[0m-215.9   \u001b[0m | \u001b[0m7.568    \u001b[0m | \u001b[0m4.422    \u001b[0m | \u001b[0m2.046    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m29       \u001b[0m | \u001b[0m-218.1   \u001b[0m | \u001b[0m5.212    \u001b[0m | \u001b[0m9.955    \u001b[0m | \u001b[0m0.6188   \u001b[0m | \u001b[0m0.04064  \u001b[0m |\n",
            "| \u001b[0m30       \u001b[0m | \u001b[0m-236.5   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m7.989    \u001b[0m | \u001b[0m1.965    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m31       \u001b[0m | \u001b[0m-226.8   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m32       \u001b[0m | \u001b[0m-774.9   \u001b[0m | \u001b[0m7.921    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m3.842    \u001b[0m | \u001b[0m0.001    \u001b[0m |\n",
            "| \u001b[0m33       \u001b[0m | \u001b[0m-883.2   \u001b[0m | \u001b[0m6.242    \u001b[0m | \u001b[0m3.566    \u001b[0m | \u001b[0m0.8619   \u001b[0m | \u001b[0m0.001    \u001b[0m |\n",
            "| \u001b[0m34       \u001b[0m | \u001b[0m-217.2   \u001b[0m | \u001b[0m5.971    \u001b[0m | \u001b[0m5.87     \u001b[0m | \u001b[0m2.414    \u001b[0m | \u001b[0m0.04734  \u001b[0m |\n",
            "| \u001b[0m35       \u001b[0m | \u001b[0m-224.6   \u001b[0m | \u001b[0m5.002    \u001b[0m | \u001b[0m6.291    \u001b[0m | \u001b[0m1.612    \u001b[0m | \u001b[0m0.0671   \u001b[0m |\n",
            "| \u001b[0m36       \u001b[0m | \u001b[0m-221.9   \u001b[0m | \u001b[0m7.049    \u001b[0m | \u001b[0m5.915    \u001b[0m | \u001b[0m1.576    \u001b[0m | \u001b[0m0.06227  \u001b[0m |\n",
            "| \u001b[0m37       \u001b[0m | \u001b[0m-218.0   \u001b[0m | \u001b[0m6.704    \u001b[0m | \u001b[0m7.228    \u001b[0m | \u001b[0m2.043    \u001b[0m | \u001b[0m0.03242  \u001b[0m |\n",
            "| \u001b[0m38       \u001b[0m | \u001b[0m-220.0   \u001b[0m | \u001b[0m6.104    \u001b[0m | \u001b[0m6.584    \u001b[0m | \u001b[0m0.6011   \u001b[0m | \u001b[0m0.06029  \u001b[0m |\n",
            "| \u001b[0m39       \u001b[0m | \u001b[0m-783.7   \u001b[0m | \u001b[0m5.767    \u001b[0m | \u001b[0m7.239    \u001b[0m | \u001b[0m3.252    \u001b[0m | \u001b[0m0.001    \u001b[0m |\n",
            "| \u001b[0m40       \u001b[0m | \u001b[0m-224.5   \u001b[0m | \u001b[0m7.641    \u001b[0m | \u001b[0m6.544    \u001b[0m | \u001b[0m2.431    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m41       \u001b[0m | \u001b[0m-218.3   \u001b[0m | \u001b[0m5.268    \u001b[0m | \u001b[0m7.767    \u001b[0m | \u001b[0m0.6295   \u001b[0m | \u001b[0m0.05541  \u001b[0m |\n",
            "| \u001b[0m42       \u001b[0m | \u001b[0m-223.9   \u001b[0m | \u001b[0m6.202    \u001b[0m | \u001b[0m8.736    \u001b[0m | \u001b[0m1.313    \u001b[0m | \u001b[0m0.09201  \u001b[0m |\n",
            "| \u001b[0m43       \u001b[0m | \u001b[0m-223.3   \u001b[0m | \u001b[0m4.127    \u001b[0m | \u001b[0m8.149    \u001b[0m | \u001b[0m1.371    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m44       \u001b[0m | \u001b[0m-218.5   \u001b[0m | \u001b[0m8.121    \u001b[0m | \u001b[0m5.085    \u001b[0m | \u001b[0m0.7093   \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m45       \u001b[0m | \u001b[0m-225.8   \u001b[0m | \u001b[0m7.602    \u001b[0m | \u001b[0m8.083    \u001b[0m | \u001b[0m1.89     \u001b[0m | \u001b[0m0.08006  \u001b[0m |\n",
            "| \u001b[95m46       \u001b[0m | \u001b[95m-213.4   \u001b[0m | \u001b[95m7.503    \u001b[0m | \u001b[95m6.393    \u001b[0m | \u001b[95m0.07997  \u001b[0m | \u001b[95m0.06078  \u001b[0m |\n",
            "| \u001b[0m47       \u001b[0m | \u001b[0m-223.3   \u001b[0m | \u001b[0m8.565    \u001b[0m | \u001b[0m7.108    \u001b[0m | \u001b[0m3.856    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m48       \u001b[0m | \u001b[0m-226.8   \u001b[0m | \u001b[0m9.043    \u001b[0m | \u001b[0m3.802    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m49       \u001b[0m | \u001b[0m-215.8   \u001b[0m | \u001b[0m0.6036   \u001b[0m | \u001b[0m8.232    \u001b[0m | \u001b[0m0.3172   \u001b[0m | \u001b[0m0.09251  \u001b[0m |\n",
            "| \u001b[0m50       \u001b[0m | \u001b[0m-220.1   \u001b[0m | \u001b[0m2.238    \u001b[0m | \u001b[0m7.755    \u001b[0m | \u001b[0m1.164    \u001b[0m | \u001b[0m0.07837  \u001b[0m |\n",
            "| \u001b[0m51       \u001b[0m | \u001b[0m-251.7   \u001b[0m | \u001b[0m6.368    \u001b[0m | \u001b[0m3.121    \u001b[0m | \u001b[0m3.609    \u001b[0m | \u001b[0m0.03568  \u001b[0m |\n",
            "| \u001b[0m52       \u001b[0m | \u001b[0m-234.0   \u001b[0m | \u001b[0m0.8258   \u001b[0m | \u001b[0m6.588    \u001b[0m | \u001b[0m1.472    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
            "| \u001b[0m53       \u001b[0m | \u001b[0m-747.0   \u001b[0m | \u001b[0m1.763    \u001b[0m | \u001b[0m6.862    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.001    \u001b[0m |\n",
            "| \u001b[0m54       \u001b[0m | \u001b[0m-218.6   \u001b[0m | \u001b[0m1.217    \u001b[0m | \u001b[0m8.864    \u001b[0m | \u001b[0m1.545    \u001b[0m | \u001b[0m0.09309  \u001b[0m |\n",
            "| \u001b[95m55       \u001b[0m | \u001b[95m-210.8   \u001b[0m | \u001b[95m2.734    \u001b[0m | \u001b[95m8.815    \u001b[0m | \u001b[95m1.578    \u001b[0m | \u001b[95m0.07193  \u001b[0m |\n",
            "=========================================================================\n",
            "Best hyperparameters for CatBoostRegressor:\n",
            "{'bagging_temperature': 2.7336210592478105, 'depth': 8.81488333292302, 'l2_leaf_reg': 1.5779257295710725, 'learning_rate': 0.07192728333846252}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(n_estimators=1000, random_state=1)\n",
        "\n",
        "# Train the model\n",
        "rfr_model.fit(X_train, Y_train)\n",
        "\n",
        "# Feature selection based on feature importance\n",
        "feature_selector = SelectFromModel(rfr_model, threshold='median')\n",
        "feature_selector.fit(X_train, Y_train)\n",
        "X_train_selected = feature_selector.transform(X_train)\n",
        "X_test_selected = feature_selector.transform(X_test)\n",
        "\n",
        "# Fit the model on selected features\n",
        "rfr_model_selected = RandomForestRegressor(n_estimators=1000, random_state=1)\n",
        "rfr_model_selected.fit(X_train_selected, Y_train)\n",
        "\n",
        "# Predict the target variable\n",
        "optimal_rfr_feature_values = X_test_selected[0]  # Use any data point from test set for prediction\n",
        "optimal_rfr_y_value = rfr_model_selected.predict([optimal_rfr_feature_values])[0]\n",
        "\n",
        "print('Optimal feature values for RandomForestRegressor:', optimal_rfr_feature_values)\n",
        "print('Predicted optimal Y value for RandomForestRegressor:', optimal_rfr_y_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5EeUB2uRRPW",
        "outputId": "04b06dda-cc68-4d76-c108-0b9ab9ed5267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for RandomForestRegressor: [9.645e+01 2.500e-01 2.000e-01 7.500e-02 7.500e-02 0.000e+00 3.000e-01\n",
            " 0.000e+00 2.500e+00 7.500e-02 1.100e+01]\n",
            "Predicted optimal Y value for RandomForestRegressor: 91.616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Initialize the CatBoostRegressor model\n",
        "catboost_model = CatBoostRegressor(learning_rate=0.05, depth=6, iterations=100, random_state=1, verbose=0)\n",
        "\n",
        "# Train the model\n",
        "catboost_model.fit(X_train, Y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = catboost_model.get_feature_importance()\n",
        "\n",
        "# Select top features based on importance\n",
        "top_features_indices = feature_importance.argsort()[-5:][::-1]  # Select top 5 features\n",
        "X_train_top_features = X_train[:, top_features_indices]\n",
        "X_test_top_features = X_test[:, top_features_indices]\n",
        "\n",
        "# Fit the model on selected features\n",
        "catboost_model_top_features = CatBoostRegressor(learning_rate=0.05, depth=6, iterations=100, random_state=1, verbose=0)\n",
        "catboost_model_top_features.fit(X_train_top_features, Y_train)\n",
        "\n",
        "# Predict the target variable\n",
        "optimal_catboost_feature_values = X_test_top_features[0]  # Use any data point from test set for prediction\n",
        "optimal_catboost_y_value = catboost_model_top_features.predict([optimal_catboost_feature_values])[0]\n",
        "\n",
        "print('Optimal feature values for CatBoostRegressor:', optimal_catboost_feature_values)\n",
        "print('Predicted optimal Y value for CatBoostRegressor:', optimal_catboost_y_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4-RwDKeSPtc",
        "outputId": "9087bd44-257a-4156-ba13-c4333817cc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal feature values for CatBoostRegressor: [9.645e+01 2.500e+00 1.100e+01 7.500e-02 7.500e-02]\n",
            "Predicted optimal Y value for CatBoostRegressor: 105.02546351595443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evolutionary_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK9OIp1LgA08",
        "outputId": "21d2b0eb-2d07-4c03-86cf-9f1ead185bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement evolutionary_search (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for evolutionary_search\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
        "\n",
        "# Define the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Define the parameter grid for feature selection using genetic algorithms\n",
        "param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize and fit EvolutionaryAlgorithmSearchCV for feature selection\n",
        "evolutionary_search = EvolutionaryAlgorithmSearchCV(estimator=rfr_model,\n",
        "                                                    params=param_grid,\n",
        "                                                    scoring='neg_mean_squared_error',\n",
        "                                                    cv=5,\n",
        "                                                    verbose=1,\n",
        "                                                    population_size=50,\n",
        "                                                    gene_mutation_prob=0.10,\n",
        "                                                    gene_crossover_prob=0.5,\n",
        "                                                    tournament_size=3,\n",
        "                                                    generations_number=10)\n",
        "evolutionary_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best estimator after feature selection\n",
        "best_rfr_model = evolutionary_search.best_estimator_\n",
        "\n",
        "# Predict the target variable\n",
        "optimal_rfr_feature_values = X_test[0]  # Use any data point from test set for prediction\n",
        "optimal_rfr_y_value = best_rfr_model.predict([optimal_rfr_feature_values])[0]\n",
        "\n",
        "print('Optimal feature values for RandomForestRegressor:', optimal_rfr_feature_values)\n",
        "print('Predicted optimal Y value for RandomForestRegressor:', optimal_rfr_y_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "tzNzas2MSRxk",
        "outputId": "9e759a15-ebe9-4617-8f03-8c13bb13cf79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'evolutionary_search'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-acd639005426>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevolutionary_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvolutionaryAlgorithmSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the RandomForestRegressor model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrfr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evolutionary_search'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Define the CatBoostRegressor model\n",
        "catboost_model = CatBoostRegressor(learning_rate=0.05, depth=6, iterations=100, random_state=1, verbose=0)\n",
        "\n",
        "# Fit the CatBoostRegressor model\n",
        "catboost_model.fit(X_train, Y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = catboost_model.get_feature_importance()\n",
        "\n",
        "# Sort features based on importance\n",
        "sorted_features_indices = feature_importance.argsort()[::-1]\n",
        "\n",
        "# Select top features based on importance\n",
        "top_features_indices = sorted_features_indices[:5]  # Select top 5 features\n",
        "X_train_top_features = X_train[:, top_features_indices]\n",
        "X_test_top_features = X_test[:, top_features_indices]\n",
        "\n",
        "# Fit the model on selected features\n",
        "catboost_model_top_features = CatBoostRegressor(learning_rate=0.05, depth=6, iterations=100, random_state=1, verbose=0)\n",
        "catboost_model_top_features.fit(X_train_top_features, Y_train)\n",
        "\n",
        "# Predict the target variable\n",
        "optimal_catboost_feature_values = X_test_top_features[0]  # Use any data point from test set for prediction\n",
        "optimal_catboost_y_value = catboost_model_top_features.predict([optimal_catboost_feature_values])[0]\n",
        "\n",
        "print('Optimal feature values for CatBoostRegressor:', optimal_catboost_feature_values)\n",
        "print('Predicted optimal Y value for CatBoostRegressor:', optimal_catboost_y_value)\n"
      ],
      "metadata": {
        "id": "q13onOSweqoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tpot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "dtrv7JHghCcZ",
        "outputId": "022d4314-7aca-4007-d437-2ae67c46168f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tpot\n",
            "  Downloading TPOT-0.12.2-py3-none-any.whl (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m81.9/87.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.11.4)\n",
            "Collecting scikit-learn>=1.4.1 (from tpot)\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deap>=1.2 (from tpot)\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting update-checker>=0.16 (from tpot)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (4.66.2)\n",
            "Collecting stopit>=1.1.1 (from tpot)\n",
            "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (2.0.3)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.4.0)\n",
            "Requirement already satisfied: xgboost>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tpot) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.1->tpot) (3.4.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.16->tpot) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->tpot) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2024.2.2)\n",
            "Building wheels for collected packages: stopit\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11938 sha256=7b28bdf41886fc9975e9e1190a13bc887a2095e8f7eff04e896b7611eb70fe36\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/f9/87/bf5b3d565c2a007b4dae9d8142dccc85a9f164e517062dd519\n",
            "Successfully built stopit\n",
            "Installing collected packages: stopit, deap, update-checker, scikit-learn, tpot\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed deap-1.4.1 scikit-learn-1.4.2 stopit-1.1.2 tpot-0.12.2 update-checker-0.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              },
              "id": "82ecd37c18ba48278a616296ebe0a3c5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from tpot import TPOTRegressor\n",
        "\n",
        "# Define the TPOTRegressor model\n",
        "tpot_model = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=1)\n",
        "\n",
        "# Fit the TPOTRegressor model\n",
        "tpot_model.fit(X_train, Y_train)\n",
        "\n",
        "# Predict the target variable\n",
        "optimal_feature_values = X_test[0]  # Use any data point from test set for prediction\n",
        "optimal_y_value = tpot_model.predict([optimal_feature_values])[0]\n",
        "\n",
        "print('Optimal feature values:', optimal_feature_values)\n",
        "print('Predicted optimal Y value:', optimal_y_value)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "DlvwEFSBes8E",
        "outputId": "c406f125-5374-4092-d1fd-163bdff6803a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3a4eacfb2146>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtpot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPOTRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define the TPOTRegressor model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtpot_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTPOTRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tpot/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtpot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPOTClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTPOTRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tpot/tpot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPOTBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassifier_config_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregressor_config_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tpot/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/impute/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMissingIndicator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_knn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_pandas_na\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_scalar_nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_get_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rfr_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Fit the RandomForestRegressor model with the training data\n",
        "rfr_model.fit(X, Y)\n",
        "\n",
        "# Define the number of random samples to generate\n",
        "num_samples = 100000  # Adjust this value based on your preference\n",
        "\n",
        "# Initialize variables to store the maximum predicted value and its corresponding feature values\n",
        "max_predicted_value = float('-inf')\n",
        "optimal_feature_values = None\n",
        "\n",
        "# Generate random samples of feature values\n",
        "for _ in range(num_samples):\n",
        "    # Randomly sample feature values from the feature space\n",
        "    features_random = [np.random.uniform(min_value, max_value) for min_value, max_value in zip(X_train.min(axis=0), X_train.max(axis=0))]\n",
        "\n",
        "    # Predict the target variable using the RandomForestRegressor model\n",
        "    predicted_value = rfr_model.predict([features_random])\n",
        "\n",
        "    # Check if the predicted value is higher than the current maximum\n",
        "    if predicted_value > max_predicted_value:\n",
        "        max_predicted_value = predicted_value\n",
        "        optimal_feature_values = features_random\n",
        "\n",
        "print('Optimal feature values for highest predicted Y value:', optimal_feature_values)\n",
        "print('Predicted highest Y value:', max_predicted_value)"
      ],
      "metadata": {
        "id": "hmL7Gm6_gaPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mk-3zZKwVe4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}